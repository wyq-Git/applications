{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 基于 MindSpore 的 DDPM 扩散模型训练与推理示例\n",
    "\n",
    "\n",
    "## 扩散模型介绍\n",
    "扩散模型是一类神经网络，其训练目标为从含噪输入中预测噪声程度略低的图像。在推理阶段，这类模型可通过迭代变换随机噪声来生成图像。\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png\" width=\"800\"/>\n",
    "    <br>\n",
    "    <em> 图片源自DDPM论文 (https://arxiv.org/abs/2006.11239). </em>\n",
    "<p>\n",
    "\n",
    "如果不熟悉其中的数学原理，不必过于担心。需要记住的核心要点是：我们的模型对应于公式中的概率分布 $p_{\\theta}(x_{t-1}|x_{t})$ (换个通俗的说法就是：预测一张噪声程度略低的图像).\n",
    "\n",
    "\n",
    "## Unet模型介绍\n",
    "\n",
    "\n",
    "大多数扩散模型都会采用 [U-net](https://arxiv.org/abs/1505.04597) 架构的某种变体，本文中我们也将使用这一架构。\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)\n",
    "\n",
    "简而言之：\n",
    "- 模型会让输入图像经过若干个 ResNet 层模块，每个模块将图像尺寸缩小一半；\n",
    "- 随后图像再经过相同数量的模块，重新将其尺寸上采样恢复；\n",
    "- 模型中设有跳跃连接（skip connections），将下采样路径上的特征层与上采样路径中对应的层连接起来。\n",
    "\n",
    "该模型的一个核心特点是，其输出图像的尺寸与输入完全一致 —— 这正是我们此处所需的特性。\n",
    "\n",
    "有意思的一点是，给图像添加噪声的操作其实非常简单，因此模型的训练可以按照如下步骤以半监督的方式进行：\n",
    "1. 从训练数据集中选取一张图像。\n",
    "2. 对该图像施加 $t$ 次随机噪声（这一步会得到上图中的 $x_{t-1}$ 和 $x_{t}$ ）\n",
    "3. 将这张含噪图像与噪声步数 $t$ 一同输入至模型 \n",
    "4. 基于模型的输出结果与含噪图像 $x_{t-1}$ 计算损失值。\n",
    "\n",
    "\n",
    "\n",
    "## 环境准备\n",
    "\n",
    "本案例使用 MindSpore 2.7.0 和 MindSpore NLP 0.5.1 实现一个完整的扩散模型训练与推理流程。\n",
    "\n",
    "运行环境为：\n",
    "- Python == 3.10\n",
    "- CANN == 8.1.RC1\n",
    "- MindSpore == 2.7.0\n",
    "- MindSpore NLP == 0.5.1\n",
    "- 设备为 Ascend，可通过环境变量控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若你在https://internstudio-ascend.intern-ai.org.cn/console/instance进行开发时，使用notebook会出现无法正常使用NPU，可进行以下步骤：\n",
    "# 进入开发机的命令窗口\n",
    "# 首先进入你的开发环境\n",
    "# conda activate ms310\n",
    "# 配置安装ascend-toolkit的环境\n",
    "# source /root/.conda/envs/ms310/Ascend/ascend-toolkit/set_env.sh\n",
    "# 在命令行中启动jupyter就可以正常使用npu了\n",
    "# jupyter lab --allow-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "!pip install mindnlp==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import mindtorch\n",
    "\n",
    "# ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"CPU\")\n",
    "# print(\"已切换至 CPU 模式\")\n",
    "\n",
    "# 目前使用测试模式PYNATIVE_MODE下的Ascend设备进行调试,后面可以改为GRAPH_MODE\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\", device_id=0)\n",
    "print(\"已切换至 Ascend 模式，设备 ID: 0\")\n",
    "import contextlib\n",
    "# 为旧版 mindtorch 补上 autograd.profiler，避免 zero_grad 中调用时报错\n",
    "if not hasattr(mindtorch.autograd, \"profiler\"):\n",
    "    class _DummyProfiler:\n",
    "        @staticmethod\n",
    "        @contextlib.contextmanager\n",
    "        def record_function(name):\n",
    "            yield\n",
    "    mindtorch.autograd.profiler = _DummyProfiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 配置训练参数\n",
    "\n",
    "在开始构建数据管道和模型之前，先集中定义训练过程中需要用到的超参数：\n",
    "\n",
    "- `image_size`：输入图像分辨率，这里使用 128×128；\n",
    "- `train_batch_size` / `eval_batch_size`：训练与评估的 batch 大小；\n",
    "- `num_epochs`：训练轮数；\n",
    "- `learning_rate`：优化器的学习率；\n",
    "- `save_image_epochs` / `save_model_epochs`：保存采样图片和模型权重的频率；\n",
    "- `output_dir`：训练产物（权重、图片）的输出目录；\n",
    "- `seed`：随机种子，保证实验可复现。\n",
    "\n",
    "这些配置会在后续的数据预处理、模型构建、训练与推理各步骤中被统一引用，方便整体调整实验设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 配置训练参数\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # 输入图像大小\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # 评估时的 batch size\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = 'fp16'  # MindSpore 中通过 amp_level 控制\n",
    "    output_dir = 'ddpm-butterflies-128-ms'\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 加载与处理数据集\n",
    "\n",
    "本节加载 HuggingFace 上的 `huggan/smithsonian_butterflies_subset` 蝴蝶数据集，并使用 MindSpore 的 `GeneratorDataset` 封装为可迭代的数据管道：\n",
    "\n",
    "1. 从 Hub 下载原始 PIL 图像；\n",
    "2. 使用自定义的 `transform` 函数将图像 Resize 到目标分辨率、归一化到 [-1, 1] 并转换为 `CHW` 格式的 numpy 数组；\n",
    "3. 通过 `ButterflyIterator` 将 HF 数据集包装成可索引的迭代器；\n",
    "4. 使用 `GeneratorDataset` + `shuffle` + `batch` 得到训练用的 `data_loader`。\n",
    "\n",
    "这一部分的输出是一个 MindSpore Dataset 对象，后续会在训练循环中按 batch 形式取出图像，并转换为 torch 张量交给模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 加载与处理数据集\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision as vision\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128# 输入图像大小\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # 评估时的 batch size\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = 'fp16'# MindSpore 中通过 amp_level 控制\n",
    "    output_dir = 'ddpm-butterflies-128-ms'\n",
    "    seed = 0\n",
    "    \n",
    "config = TrainingConfig()    \n",
    "\n",
    "\n",
    "# 1. 加载 HF 数据集\n",
    "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
    "\n",
    "# 2. 定义预处理操作 (使用 MindSpore Vision 算子)\n",
    "def transform(data):\n",
    "    # data 是一个字典，包含 'image' 键\n",
    "    image = data['image']\n",
    "    \n",
    "    # 预处理流程：Resize -> RandomHorizontalFlip -> ToTensor -> Normalize\n",
    "    # 注意：MindSpore 的 HWC -> CHW 转换通常在 ToTensor 或后续处理中\n",
    "    # 这里为了简单，可以用 numpy/PIL 处理完直接转 Tensor\n",
    "    image = image.resize((config.image_size, config.image_size))\n",
    "    # ... 其他增强操作 ...\n",
    "    \n",
    "    # 归一化到 [-1, 1] 并转为 CHW 格式\n",
    "    image = np.array(image) / 127.5 - 1.0\n",
    "    image = image.transpose(2, 0, 1).astype(np.float32)\n",
    "    return image\n",
    "\n",
    "# 3. 封装为 MindSpore Dataset\n",
    "class ButterflyIterator:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataset[index]\n",
    "        return transform(item),  # 返回 tuple\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# 创建数据迭代器\n",
    "data_loader = ds.GeneratorDataset(ButterflyIterator(dataset), column_names=[\"image\"])\n",
    "data_loader = data_loader.shuffle(buffer_size=1000)\n",
    "data_loader = data_loader.batch(config.train_batch_size)\n",
    "\n",
    "# 打印一下看看形状\n",
    "for item in data_loader.create_dict_iterator():\n",
    "    print(item['image'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 定义扩散模型与噪声调度器\n",
    "\n",
    "这一节中，我们使用 MindNLP 中封装的 diffusers 接口来构建整个扩散模型：\n",
    "\n",
    "- `DDPMScheduler`：实现 DDPM 的噪声调度逻辑，负责前向加噪和反向去噪过程中的系数计算；\n",
    "- `UNet2DModel`：时间条件 U-Net，用于在给定噪声图像和时间步 $t$ 的情况下预测噪声；\n",
    "- `model.to(\"npu:0\")`：将模型移动到 Ascend NPU 上进行加速训练。\n",
    "\n",
    "模型结构（`block_out_channels`、`down_block_types`、`up_block_types`）与 HuggingFace 官方 `training_example.ipynb` 保持一致，\n",
    "确保容量足以在 128×128 分辨率上学习蝴蝶图像分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 定义模型与调度器\n",
    "\n",
    "from mindnlp.diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "\n",
    "# 创建 Scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# 创建 UNet 模型\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\", \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\",\n",
    "        \"UpBlock2D\", \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "model.to(\"npu:0\") \n",
    "\n",
    "print(\"模型已成功移动到 NPU\")\n",
    "# 打印模型结构确认\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印一下看看参数是否正常\n",
    "params = model.trainable_params()\n",
    "print(f\"参数数量: {len(params)}\")\n",
    "print(f\"第一个参数的类型: {type(params[0])}\") \n",
    "# 这里应该输出 <class 'mindspore.common.parameter.Parameter'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 定义损失函数与优化器\n",
    "\n",
    "- `MSELoss`：让模型在所有时间步上预测的噪声尽可能接近真实噪声；\n",
    "- `AdamW`：带权重衰减的 Adam 优化器，适合训练 U-Net 这类较深的网络；\n",
    "- `device`：通过 `torch.device(\"npu:0\")` 将模型与张量统一放在 Ascend NPU 上，由 MindNLP/mindtorch 做后端调度。\n",
    "\n",
    "之后的训练步骤会基于这套损失与优化器进行标准的 `loss.backward()` + `optimizer.step()` 迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 定义损失函数与优化器（由 mindnlp/mindhf 代理到 MindSpore）\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"npu:0\" if hasattr(torch, \"npu\") else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "在这一步，我们实现一个单步的训练函数 `train_step`：\n",
    "\n",
    "1. 从干净图像 `clean_images` 中采样同形状的高斯噪声 `noise`；\n",
    "2. 从调度器的时间步范围中随机采样一个整型向量 `timesteps`；\n",
    "3. 调用 `noise_scheduler.add_noise` 得到噪声图像 `noisy_images`；\n",
    "4. 通过 U-Net 模型预测噪声 `noise_pred = model(noisy_images, timesteps).sample`；\n",
    "5. 使用 `MSELoss` 计算预测噪声与真实噪声之间的距离；\n",
    "6. 调用 `loss.backward()` 和 `optimizer.step()` 完成一次参数更新。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写训练流程 (Forward & Backward) \n",
    "\n",
    "def train_step(clean_images: torch.Tensor):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bs = clean_images.shape[0]\n",
    "    noise = torch.randn_like(clean_images)\n",
    "\n",
    "    # 使用 scheduler 的 config.num_train_timesteps\n",
    "    timesteps = torch.randint(\n",
    "        0,\n",
    "        noise_scheduler.config.num_train_timesteps,\n",
    "        (bs,),\n",
    "        device=device,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "\n",
    "    noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "    noise_pred = model(noisy_images, timesteps).sample\n",
    "    loss = loss_fn(noise_pred, noise)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 定义评估与采样辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估与采样辅助函数\n",
    "from PIL import Image\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.Generator(device=device).manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "    samples_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    image_grid.save(os.path.join(samples_dir, f\"epoch_{epoch:04d}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "训练循环整体流程：\n",
    "\n",
    "1. 遍历 `num_epochs` 个训练轮次；\n",
    "2. 每个 epoch 中从 `data_loader` 取出一个 batch，转为 NPU 上的 torch 张量；\n",
    "3. 调用 `train_step` 完成一次前向 + 反向 + 参数更新；\n",
    "4. 使用 `tqdm` 展示训练进度和当前 loss；\n",
    "5. 周期性使用 `DDPMPipeline` 从纯噪声采样，保存训练过程中的生成图片快照；\n",
    "6. 周期性保存当前模型权重，便于后续单独加载做推理。\n",
    "\n",
    "这样可以方便地观察 loss 曲线以及生成图像质量的演化趋势，同时保留中间检查点用于调试和复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 启动训练循环并在训练过程中保存模型与采样图片\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.set_train(True)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    step_loss = []\n",
    "    with tqdm(total=data_loader.get_dataset_size()) as progress_bar:\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch in data_loader.create_dict_iterator():\n",
    "            # 从 MindSpore Dataset 中取出的数据先转为 numpy，再转为 torch.Tensor\n",
    "            images_np = batch['image']\n",
    "            images_np = np.array(images_np)\n",
    "            clean_images = torch.tensor(images_np, dtype=torch.float32, device=device)\n",
    "\n",
    "            # 执行一步训练（纯 torch 接口，由 mindnlp/mindhf 代理到 MindSpore/Ascend）\n",
    "            loss = train_step(clean_images)\n",
    "            step_loss.append(loss.cpu().item())\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(loss=loss.cpu().item())\n",
    "    \n",
    "    avg_loss = float(np.mean(step_loss))\n",
    "    print(f\"Epoch {epoch} finished. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 周期性保存采样图片（从纯噪声生成），参考 training_example.ipynb\n",
    "    if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "        pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "        evaluate(config, epoch + 1, pipeline)\n",
    "\n",
    "    # 周期性保存模型权重\n",
    "    if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        ckpt_path = os.path.join(config.output_dir, \"unet_ddpm_mindnlp_hf.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 推理与评估：从纯噪声生成图像\n",
    "\n",
    "在训练完成后，可以单独加载最新的模型权重，从纯噪声出发执行 DDPM 反向采样，\n",
    "生成一组蝴蝶图像。这里同样使用 `DDPMPipeline` 进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 推理：从纯噪声采样最终图像网格\n",
    "\n",
    "# 重新构建模型并加载最新权重（确保推理和训练结构一致）\n",
    "inference_model = UNet2DModel(\n",
    "    sample_size=config.image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\", \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\",\n",
    "        \"UpBlock2D\", \"UpBlock2D\"\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "ckpt_path = os.path.join(config.output_dir, \"unet_ddpm_mindnlp_hf.pt\")\n",
    "state_dict = torch.load(ckpt_path, map_location=device)\n",
    "inference_model.load_state_dict(state_dict)\n",
    "inference_model.eval()\n",
    "\n",
    "# 使用新的调度器与 pipeline 做推理\n",
    "inference_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "inference_pipeline = DDPMPipeline(unet=inference_model, scheduler=inference_scheduler)\n",
    "\n",
    "images = inference_pipeline(\n",
    "    batch_size=config.eval_batch_size,\n",
    "    generator=torch.Generator(device=device).manual_seed(config.seed),\n",
    ").images\n",
    "\n",
    "final_grid = make_grid(images, rows=4, cols=4)\n",
    "final_path = os.path.join(config.output_dir, \"final_samples_grid.png\")\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "final_grid.save(final_path)\n",
    "final_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "[1] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. Advances in neural information processing systems. 2020;33:6840-51.\n",
    "\n",
    "[2] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. InInternational Conference on Medical image computing and computer-assisted intervention 2015 Oct 5 (pp. 234-241)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ms310)",
   "language": "python",
   "name": "ms310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

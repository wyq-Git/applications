{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c0ef8c-29c2-4fac-be6f-0f230c642a68",
   "metadata": {},
   "source": [
    "# 基于 MindSpore 的 DDPM 扩散模型训练与推理示例\n",
    "\n",
    "本 Notebook 使用 MindSpore 2.7.0 和 MindSpore NLP 0.5.1 实现一个完整的扩散模型训练与推理流程。\n",
    "\n",
    "主要内容包括：\n",
    "- 使用 `huggan/smithsonian_butterflies_subset` 蝴蝶图像数据集\n",
    "- 实现 DDPM 噪声调度器\n",
    "- 构建时间条件 U-Net 噪声预测网络\n",
    "- 完成模型训练与从噪声生成图像的推理流程\n",
    "\n",
    "运行环境假定：\n",
    "- Python ≥ 3.9 且 < 3.12\n",
    "- CANN ≥ 8.1.RC1（推荐 8.3.RC1）\n",
    "- MindSpore ≥ 2.7.0\n",
    "- MindSpore NLP == 0.5.1\n",
    "- 设备为 Ascend，可通过环境变量控制\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/root/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/root/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/root/.local/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "[WARNING] ME(3198:281472997273632,MainProcess):2025-12-15-23:05:54.274.000 [mindspore/context.py:1412] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n",
      "[WARNING] ME(3198:281472997273632,MainProcess):2025-12-15-23:05:54.283.000 [mindspore/context.py:1412] For 'context.set_context', the parameter 'device_id' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已切换至 Ascend 模式，设备 ID: 0\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "import mindtorch\n",
    "\n",
    "# 【修改】将 device_target 改为 \"CPU\"\n",
    "# 注意：CPU 模式通常不需要指定 device_id\n",
    "# ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"CPU\")\n",
    "\n",
    "# print(\"已切换至 CPU 模式\")\n",
    "# 目前使用测试模式PYNATIVE_MODE下的Ascend设备进行调试,后面可以改为GRAPH_MODE\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\", device_id=0)\n",
    "print(\"已切换至 Ascend 模式，设备 ID: 0\")\n",
    "import contextlib\n",
    "# 为旧版 mindtorch 补上 autograd.profiler，避免 zero_grad 中调用时报错\n",
    "if not hasattr(mindtorch.autograd, \"profiler\"):\n",
    "    class _DummyProfiler:\n",
    "        @staticmethod\n",
    "        @contextlib.contextmanager\n",
    "        def record_function(name):\n",
    "            yield\n",
    "    mindtorch.autograd.profiler = _DummyProfiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_training_config_intro",
   "metadata": {},
   "source": [
    "## 2. 配置训练参数\n",
    "\n",
    "在开始构建数据管道和模型之前，先集中定义训练过程中需要用到的超参数：\n",
    "\n",
    "- `image_size`：输入图像分辨率，这里使用 128×128；\n",
    "- `train_batch_size` / `eval_batch_size`：训练与评估的 batch 大小；\n",
    "- `num_epochs`：训练轮数；\n",
    "- `learning_rate`：优化器的学习率；\n",
    "- `save_image_epochs` / `save_model_epochs`：保存采样图片和模型权重的频率；\n",
    "- `output_dir`：训练产物（权重、图片）的输出目录；\n",
    "- `seed`：随机种子，保证实验可复现。\n",
    "\n",
    "这些配置会在后续的数据预处理、模型构建、训练与推理各步骤中被统一引用，方便整体调整实验设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c37466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 配置训练参数\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # 输入图像大小\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # 评估时的 batch size\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = 'fp16'  # MindSpore 中通过 amp_level 控制\n",
    "    output_dir = 'ddpm-butterflies-128-ms'\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_dataset_intro",
   "metadata": {},
   "source": [
    "## 3. 加载与处理数据集\n",
    "\n",
    "本节加载 HuggingFace 上的 `huggan/smithsonian_butterflies_subset` 蝴蝶数据集，并使用 MindSpore 的 `GeneratorDataset` 封装为可迭代的数据管道：\n",
    "\n",
    "1. 从 Hub 下载原始 PIL 图像；\n",
    "2. 使用自定义的 `transform` 函数将图像 Resize 到目标分辨率、归一化到 [-1, 1] 并转换为 `CHW` 格式的 numpy 数组；\n",
    "3. 通过 `ButterflyIterator` 将 HF 数据集包装成可索引的迭代器；\n",
    "4. 使用 `GeneratorDataset` + `shuffle` + `batch` 得到训练用的 `data_loader`。\n",
    "\n",
    "这一部分的输出是一个 MindSpore Dataset 对象，后续会在训练循环中按 batch 形式取出图像，并转换为 torch 张量交给模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2f2e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/ms310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindtorch.Size([16, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# 3. 加载与处理数据集\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision as vision\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128# 输入图像大小\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # 评估时的 batch size\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = 'fp16'# MindSpore 中通过 amp_level 控制\n",
    "    output_dir = 'ddpm-butterflies-128-ms'\n",
    "    seed = 0\n",
    "    \n",
    "config = TrainingConfig()    \n",
    "\n",
    "\n",
    "# 1. 加载 HF 数据集\n",
    "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
    "\n",
    "# 2. 定义预处理操作 (使用 MindSpore Vision 算子)\n",
    "def transform(data):\n",
    "    # data 是一个字典，包含 'image' 键\n",
    "    image = data['image']\n",
    "    \n",
    "    # 预处理流程：Resize -> RandomHorizontalFlip -> ToTensor -> Normalize\n",
    "    # 注意：MindSpore 的 HWC -> CHW 转换通常在 ToTensor 或后续处理中\n",
    "    # 这里为了简单，可以用 numpy/PIL 处理完直接转 Tensor\n",
    "    image = image.resize((config.image_size, config.image_size))\n",
    "    # ... 其他增强操作 ...\n",
    "    \n",
    "    # 归一化到 [-1, 1] 并转为 CHW 格式\n",
    "    image = np.array(image) / 127.5 - 1.0\n",
    "    image = image.transpose(2, 0, 1).astype(np.float32)\n",
    "    return image\n",
    "\n",
    "# 3. 封装为 MindSpore Dataset\n",
    "class ButterflyIterator:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataset[index]\n",
    "        return transform(item),  # 返回 tuple\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# 创建数据迭代器\n",
    "data_loader = ds.GeneratorDataset(ButterflyIterator(dataset), column_names=[\"image\"])\n",
    "data_loader = data_loader.shuffle(buffer_size=1000)\n",
    "data_loader = data_loader.batch(config.train_batch_size)\n",
    "\n",
    "# 打印一下看看形状\n",
    "for item in data_loader.create_dict_iterator():\n",
    "    print(item['image'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf929a66",
   "metadata": {},
   "source": [
    "## 定义扩散模型\n",
    "\n",
    "在此，我们搭建扩散模型。扩散模型是一类神经网络，其训练目标为从含噪输入中预测噪声程度略低的图像。在推理阶段，这类模型可通过迭代变换随机噪声来生成图像。\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png\" width=\"800\"/>\n",
    "    <br>\n",
    "    <em> 图片源自DDPM论文 (https://arxiv.org/abs/2006.11239). </em>\n",
    "<p>\n",
    "\n",
    "如果不熟悉其中的数学原理，不必过于担心。需要记住的核心要点是：我们的模型对应于公式中的概率分布 $p_{\\theta}(x_{t-1}|x_{t})$ (换个通俗的说法就是：预测一张噪声程度略低的图像).\n",
    "\n",
    "有意思的一点是，给图像添加噪声的操作其实非常简单，因此模型的训练可以按照如下步骤以半监督的方式进行：\n",
    "1. 从训练数据集中选取一张图像。\n",
    "2. 对该图像施加 $t$ 次随机噪声（这一步会得到上图中的 $x_{t-1}$ 和 $x_{t}$ ）\n",
    "3. 将这张含噪图像与噪声步数 $t$ 一同输入至模型 \n",
    "4. 基于模型的输出结果与含噪图像 $x_{t-1}$ 计算损失值。\n",
    "\n",
    "随后，我们就可以采用梯度下降法，并重复上述流程多次以完成模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_model_scheduler_intro",
   "metadata": {},
   "source": [
    "## 4. 定义扩散模型与噪声调度器\n",
    "\n",
    "这一节中，我们使用 MindNLP 中封装的 diffusers 接口来构建整个扩散模型：\n",
    "\n",
    "- `DDPMScheduler`：实现 DDPM 的噪声调度逻辑，负责前向加噪和反向去噪过程中的系数计算；\n",
    "- `UNet2DModel`：时间条件 U-Net，用于在给定噪声图像和时间步 $t$ 的情况下预测噪声；\n",
    "- `model.to(\"npu:0\")`：将模型移动到 Ascend NPU 上进行加速训练。\n",
    "\n",
    "模型结构（`block_out_channels`、`down_block_types`、`up_block_types`）与 HuggingFace 官方 `training_example.ipynb` 保持一致，\n",
    "确保容量足以在 128×128 分辨率上学习蝴蝶图像分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dead7c4",
   "metadata": {},
   "source": [
    "大多数扩散模型都会采用 [U-net](https://arxiv.org/abs/1505.04597) 架构的某种变体，本文中我们也将使用这一架构。\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)\n",
    "\n",
    "简而言之：\n",
    "- 模型会让输入图像经过若干个 ResNet 层模块，每个模块将图像尺寸缩小一半；\n",
    "- 随后图像再经过相同数量的模块，重新将其尺寸上采样恢复；\n",
    "- 模型中设有跳跃连接（skip connections），将下采样路径上的特征层与上采样路径中对应的层连接起来。\n",
    "\n",
    "该模型的一个核心特点是，其输出图像的尺寸与输入完全一致 —— 这正是我们此处所需的特性。\n",
    "Diffusers 库为我们提供了便捷的 UNet2DModel 类，可在 PyTorch 中快速构建上述所需架构。\n",
    "\n",
    "接下来，我们针对目标图像尺寸创建一个 U-net。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0529cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modular Diffusers is currently an experimental feature under active development. The API is subject to breaking changes in future releases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n",
      "模型已成功移动到 NPU\n",
      "UNet2DModel(\n",
      "  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (time_proj): Timesteps()\n",
      "  (time_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (down_blocks): ModuleList(\n",
      "    (0-1): 2 x DownBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DownBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DownBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): AttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): DownBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_blocks): ModuleList(\n",
      "    (0): UpBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-2): 3 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): AttnUpBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): UpBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): UpBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): UpBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): UpBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-2): 3 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlock2D(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Attention(\n",
      "        (group_norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (time_emb_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 4. 定义模型与调度器\n",
    "\n",
    "from mindnlp.diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline\n",
    "\n",
    "# 创建 Scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# 创建 UNet 模型\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\", \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\",\n",
    "        \"UpBlock2D\", \"UpBlock2D\"\n",
    "    )\n",
    ")\n",
    "model.to(\"npu:0\") \n",
    "\n",
    "print(\"模型已成功移动到 NPU\")\n",
    "# 打印模型结构确认\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1409c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数数量: 450\n",
      "第一个参数的类型: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 打印一下看看参数是否正常\n",
    "params = model.trainable_params()\n",
    "print(f\"参数数量: {len(params)}\")\n",
    "print(f\"第一个参数的类型: {type(params[0])}\") \n",
    "# 这里应该输出 <class 'mindspore.common.parameter.Parameter'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_loss_opt_intro",
   "metadata": {},
   "source": [
    "## 5. 定义损失函数与优化器\n",
    "\n",
    "和原始 PyTorch 示例一样，我们在这里选择：\n",
    "\n",
    "- `MSELoss`：让模型在所有时间步上预测的噪声尽可能接近真实噪声；\n",
    "- `AdamW`：带权重衰减的 Adam 优化器，适合训练 U-Net 这类较深的网络；\n",
    "- `device`：通过 `torch.device(\"npu:0\")` 将模型与张量统一放在 Ascend NPU 上，由 MindNLP/mindtorch 做后端调度。\n",
    "\n",
    "之后的训练步骤会基于这套损失与优化器进行标准的 `loss.backward()` + `optimizer.step()` 迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971134de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 定义损失函数与优化器（由 mindnlp/mindhf 代理到 MindSpore）\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"npu:0\" if hasattr(torch, \"npu\") else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0084358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 编写训练流程 (Forward & Backward) \n",
    "\n",
    "def train_step(clean_images: torch.Tensor):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bs = clean_images.shape[0]\n",
    "    noise = torch.randn_like(clean_images)\n",
    "\n",
    "    # 使用 scheduler 的 config.num_train_timesteps\n",
    "    timesteps = torch.randint(\n",
    "        0,\n",
    "        noise_scheduler.config.num_train_timesteps,\n",
    "        (bs,),\n",
    "        device=device,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "\n",
    "    noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "    noise_pred = model(noisy_images, timesteps).sample\n",
    "    loss = loss_fn(noise_pred, noise)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_train_step_intro",
   "metadata": {},
   "source": [
    "在这一步，我们实现一个单步的训练函数 `train_step`：\n",
    "\n",
    "1. 从干净图像 `clean_images` 中采样同形状的高斯噪声 `noise`；\n",
    "2. 从调度器的时间步范围中随机采样一个整型向量 `timesteps`；\n",
    "3. 调用 `noise_scheduler.add_noise` 得到噪声图像 `noisy_images`；\n",
    "4. 通过 U-Net 模型预测噪声 `noise_pred = model(noisy_images, timesteps).sample`；\n",
    "5. 使用 `MSELoss` 计算预测噪声与真实噪声之间的距离；\n",
    "6. 调用 `loss.backward()` 和 `optimizer.step()` 完成一次参数更新。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 评估与采样辅助函数\n",
    "from PIL import Image\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.Generator(device=device).manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "    samples_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    image_grid.save(os.path.join(samples_dir, f\"epoch_{epoch:04d}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e8f9f-9c76-4aa3-8af3-27e30afa0716",
   "metadata": {},
   "source": [
    "#### 8. 推理与评估\n",
    "\n",
    "实现从纯噪声生成图像的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09bca17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:15<00:00,  4.18it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished. Avg Loss: 0.3609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.49it/s, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Avg Loss: 0.3603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Avg Loss: 0.3604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.49it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Avg Loss: 0.3620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.45it/s, loss=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished. Avg Loss: 0.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.46it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished. Avg Loss: 0.3626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished. Avg Loss: 0.3622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished. Avg Loss: 0.3610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished. Avg Loss: 0.3616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.50it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished. Avg Loss: 0.3632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:55<00:00, 17.92it/s]\n",
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.47it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished. Avg Loss: 0.3626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.46it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 finished. Avg Loss: 0.3624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.46it/s, loss=0.355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 finished. Avg Loss: 0.3603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 finished. Avg Loss: 0.3608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.50it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 finished. Avg Loss: 0.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.47it/s, loss=0.378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 finished. Avg Loss: 0.3623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.50it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 finished. Avg Loss: 0.3614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 finished. Avg Loss: 0.3619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 finished. Avg Loss: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.47it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 finished. Avg Loss: 0.3604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:56<00:00, 17.83it/s]\n",
      "Epoch 20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.43it/s, loss=0.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 finished. Avg Loss: 0.3619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.40it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 finished. Avg Loss: 0.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.46it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 finished. Avg Loss: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.45it/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 finished. Avg Loss: 0.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.44it/s, loss=0.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 finished. Avg Loss: 0.3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 finished. Avg Loss: 0.3609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.43it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 finished. Avg Loss: 0.3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.50it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 finished. Avg Loss: 0.3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.44it/s, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 finished. Avg Loss: 0.3614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 finished. Avg Loss: 0.3597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:56<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ddpm-butterflies-128-ms/unet_ddpm_mindnlp_hf.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.33it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 finished. Avg Loss: 0.3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.35it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 finished. Avg Loss: 0.3595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 finished. Avg Loss: 0.3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.33it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 finished. Avg Loss: 0.3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.33it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 finished. Avg Loss: 0.3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:13<00:00,  4.57it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 finished. Avg Loss: 0.3614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:13<00:00,  4.55it/s, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 finished. Avg Loss: 0.3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.42it/s, loss=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 finished. Avg Loss: 0.3619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.44it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 finished. Avg Loss: 0.3606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.40it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 finished. Avg Loss: 0.3608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.84it/s]\n",
      "Epoch 40: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.46it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 finished. Avg Loss: 0.3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.39it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 finished. Avg Loss: 0.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:13<00:00,  4.52it/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 finished. Avg Loss: 0.3589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.35it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 finished. Avg Loss: 0.3613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.41it/s, loss=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 finished. Avg Loss: 0.3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:13<00:00,  4.55it/s, loss=0.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 finished. Avg Loss: 0.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.46it/s, loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 finished. Avg Loss: 0.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.44it/s, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 finished. Avg Loss: 0.3608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.48it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 finished. Avg Loss: 0.3592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:14<00:00,  4.38it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 finished. Avg Loss: 0.3608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:03<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ddpm-butterflies-128-ms/unet_ddpm_mindnlp_hf.pt\n"
     ]
    }
   ],
   "source": [
    "# 8. 启动训练循环并在训练过程中保存模型与采样图片\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.set_train(True)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    step_loss = []\n",
    "    with tqdm(total=data_loader.get_dataset_size()) as progress_bar:\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch in data_loader.create_dict_iterator():\n",
    "            # 从 MindSpore Dataset 中取出的数据先转为 numpy，再转为 torch.Tensor\n",
    "            images_np = batch['image']\n",
    "            images_np = np.array(images_np)\n",
    "            clean_images = torch.tensor(images_np, dtype=torch.float32, device=device)\n",
    "\n",
    "            # 执行一步训练（纯 torch 接口，由 mindnlp/mindhf 代理到 MindSpore/Ascend）\n",
    "            loss = train_step(clean_images)\n",
    "            step_loss.append(loss.cpu().item())\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(loss=loss.cpu().item())\n",
    "    \n",
    "    avg_loss = float(np.mean(step_loss))\n",
    "    print(f\"Epoch {epoch} finished. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 周期性保存采样图片（从纯噪声生成），参考 training_example.ipynb\n",
    "    if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "        pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "        evaluate(config, epoch + 1, pipeline)\n",
    "\n",
    "    # 周期性保存模型权重\n",
    "    if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        ckpt_path = os.path.join(config.output_dir, \"unet_ddpm_mindnlp_hf.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_training_loop_intro",
   "metadata": {},
   "source": [
    "训练循环整体流程：\n",
    "\n",
    "1. 遍历 `num_epochs` 个训练轮次；\n",
    "2. 每个 epoch 中从 `data_loader` 取出一个 batch，转为 NPU 上的 torch 张量；\n",
    "3. 调用 `train_step` 完成一次前向 + 反向 + 参数更新；\n",
    "4. 使用 `tqdm` 展示训练进度和当前 loss；\n",
    "5. 周期性使用 `DDPMPipeline` 从纯噪声采样，保存训练过程中的生成图片快照；\n",
    "6. 周期性保存当前模型权重，便于后续单独加载做推理。\n",
    "\n",
    "这样可以方便地观察 loss 曲线以及生成图像质量的演化趋势，同时保留中间检查点用于调试和复现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_title",
   "metadata": {},
   "source": [
    "## 9. 推理与评估：从纯噪声生成图像\n",
    "\n",
    "在训练完成后，可以单独加载最新的模型权重，从纯噪声出发执行 DDPM 反向采样，\n",
    "生成一组蝴蝶图像。这里同样使用 `DDPMPipeline` 进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inference_step",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:59<00:00, 16.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ddpm-butterflies-128-ms/final_samples_grid.png'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. 推理：从纯噪声采样最终图像网格\n",
    "\n",
    "# 重新构建模型并加载最新权重（确保推理和训练结构一致）\n",
    "inference_model = UNet2DModel(\n",
    "    sample_size=config.image_size,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\", \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\",\n",
    "        \"UpBlock2D\", \"UpBlock2D\"\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "ckpt_path = os.path.join(config.output_dir, \"unet_ddpm_mindnlp_hf.pt\")\n",
    "state_dict = torch.load(ckpt_path, map_location=device)\n",
    "inference_model.load_state_dict(state_dict)\n",
    "inference_model.eval()\n",
    "\n",
    "# 使用新的调度器与 pipeline 做推理\n",
    "inference_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "inference_pipeline = DDPMPipeline(unet=inference_model, scheduler=inference_scheduler)\n",
    "\n",
    "images = inference_pipeline(\n",
    "    batch_size=config.eval_batch_size,\n",
    "    generator=torch.Generator(device=device).manual_seed(config.seed),\n",
    ").images\n",
    "\n",
    "final_grid = make_grid(images, rows=4, cols=4)\n",
    "final_path = os.path.join(config.output_dir, \"final_samples_grid.png\")\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "final_grid.save(final_path)\n",
    "final_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27c015-b5bd-4060-8fcc-85f3e52b2f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f83bd9-6608-478b-93ef-0d199bce7f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ms310)",
   "language": "python",
   "name": "ms310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
